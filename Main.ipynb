{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karandomguy/Phishing-Email-Detection/blob/dev/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWc5Y2MpdF8T",
        "outputId": "dac552b8-6608-4947-9d0b-9be4fdfdc1af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hoa-UezaOAjH",
        "outputId": "29adc178-b27c-4862-a21e-020f868595aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: patool in /usr/local/lib/python3.10/dist-packages (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install patool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "yI8BM838O1Nn",
        "outputId": "1325d70d-2a15-483e-91c6-6b5048c7f840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patool: Extracting /content/gdrive/MyDrive/phishing_url_model.zip ...\n",
            "patool: running /usr/bin/7z x -o./Unpack_frz4lm6w -- /content/gdrive/MyDrive/phishing_url_model.zip\n",
            "patool: ... /content/gdrive/MyDrive/phishing_url_model.zip extracted to `best_pipeline_url.pkl'.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'best_pipeline_url.pkl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "import patoolib as pl\n",
        "pl.extract_archive(\"/content/gdrive/MyDrive/phishing_url_model.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKGtKaCxeTWe",
        "outputId": "bdcf84d3-695e-4089-8e29-11aabb5d1e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ],
      "source": [
        "%cd /content/gdrive/MyDrive/\n",
        "!cp best_pipeline_tfidf.pkl /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "A7Cvx7M4Lk6i"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def check_url(text):\n",
        "    url_pattern = \"[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)\"\n",
        "    urls = re.findall(url_pattern, text)\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CM7eOR1NMvFV"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "from googlesearch import search\n",
        "\n",
        "def having_ip_address(url):\n",
        "    match = re.search(\n",
        "        '(([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\.'\n",
        "        '([01]?\\d\\d?|2[0-4]\\d|25[0-5])\\/)|'  # IPv4\n",
        "        '((0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\.(0x[0-9a-fA-F]{1,2})\\/)' # IPv4 in hexadecimal\n",
        "        '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}', url)  # Ipv6\n",
        "    if match:\n",
        "        # print match.group()\n",
        "        return 1\n",
        "    else:\n",
        "        # print 'No matching pattern found'\n",
        "        return 0\n",
        "\n",
        "def abnormal_url(url):\n",
        "    hostname = urlparse(url).hostname\n",
        "    hostname = str(hostname)\n",
        "    match = re.search(hostname, url)\n",
        "    if match:\n",
        "        # print match.group()\n",
        "        return 1\n",
        "    else:\n",
        "        # print 'No matching pattern found'\n",
        "        return 0\n",
        "\n",
        "def google_index(url):\n",
        "    site = search(url, 5)\n",
        "    return 1 if site else 0\n",
        "\n",
        "def count_dot(url):\n",
        "    count_dot = url.count('.')\n",
        "    return count_dot\n",
        "\n",
        "def count_www(url):\n",
        "    url.count('www')\n",
        "    return url.count('www')\n",
        "\n",
        "def count_atrate(url):\n",
        "    return url.count('@')\n",
        "\n",
        "def no_of_dir(url):\n",
        "    urldir = urlparse(url).path\n",
        "    return urldir.count('/')\n",
        "\n",
        "def no_of_embed(url):\n",
        "    urldir = urlparse(url).path\n",
        "    return urldir.count('//')\n",
        "\n",
        "def shortening_service(url):\n",
        "    match = re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
        "                      'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
        "                      'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
        "                      'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
        "                      'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
        "                      'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
        "                      'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|'\n",
        "                      'tr\\.im|link\\.zip\\.net',\n",
        "                      url)\n",
        "    if match:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def count_https(url):\n",
        "    return url.count('https')\n",
        "\n",
        "def count_http(url):\n",
        "    return url.count('http')\n",
        "\n",
        "def count_per(url):\n",
        "    return url.count('%')\n",
        "\n",
        "def count_ques(url):\n",
        "    return url.count('?')\n",
        "\n",
        "def count_hyphen(url):\n",
        "    return url.count('-')\n",
        "\n",
        "def count_equal(url):\n",
        "    return url.count('=')\n",
        "\n",
        "def url_length(url):\n",
        "    return len(str(url))\n",
        "\n",
        "def hostname_length(url):\n",
        "    return len(urlparse(url).netloc)\n",
        "\n",
        "def suspicious_words(url):\n",
        "    match = re.search('PayPal|login|signin|bank|account|update|free|lucky|service|bonus|ebayisapi|webscr',\n",
        "                      url)\n",
        "    if match:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def digit_count(url):\n",
        "    digits = 0\n",
        "    for i in url:\n",
        "        if i.isnumeric():\n",
        "            digits = digits + 1\n",
        "    return digits\n",
        "\n",
        "def letter_count(url):\n",
        "    letters = 0\n",
        "    for i in url:\n",
        "        if i.isalpha():\n",
        "            letters = letters + 1\n",
        "    return letters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tld"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x_W-Ufb5YKP",
        "outputId": "350ac030-85d4-4f6b-88aa-82bb96cce70a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tld in /usr/local/lib/python3.10/dist-packages (0.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.parse import urlparse\n",
        "from tld import get_tld\n",
        "import os.path\n",
        "\n",
        "def fd_length(url):\n",
        "    urlpath= urlparse(url).path\n",
        "    try:\n",
        "        return len(urlpath.split('/')[1])\n",
        "    except:\n",
        "        return 0\n",
        "\n",
        "def get_tld_length(url):\n",
        "    try:\n",
        "        tld = get_tld(url, fail_silently=True)\n",
        "        return len(tld)\n",
        "    except:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "LYLlpvvZ5On1"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "grpzPEnnQdCn"
      },
      "outputs": [],
      "source": [
        "def feature_extractor(url):\n",
        "    features = []\n",
        "\n",
        "    # Feature 1: Having IP Address\n",
        "    ip_address_feature = having_ip_address(url)\n",
        "    features.append(ip_address_feature)\n",
        "\n",
        "    # Feature 2: Abnormal URL\n",
        "    abnormal_url_feature=abnormal_url(url)\n",
        "    features.append(abnormal_url_feature)\n",
        "\n",
        "    # Feature 3: Count Dot\n",
        "    dot_count_feature = count_dot(url)\n",
        "    features.append(dot_count_feature)\n",
        "\n",
        "    # Feature 4: Count www\n",
        "    www_count_feature = count_www(url)\n",
        "    features.append(www_count_feature)\n",
        "\n",
        "    # Feature 5: Count @\n",
        "    at_count_feature = count_atrate(url)\n",
        "    features.append(at_count_feature)\n",
        "\n",
        "    # Feature 6: Number of Directories\n",
        "    dir_count_feature = no_of_dir(url)\n",
        "    features.append(dir_count_feature)\n",
        "\n",
        "    # Feature 7: Number of Embedded Directories\n",
        "    embed_dir_count_feature = no_of_embed(url)\n",
        "    features.append(embed_dir_count_feature)\n",
        "\n",
        "    # Feature 8: Shortening Service\n",
        "    shortening_service_feature = shortening_service(url)\n",
        "    features.append(shortening_service_feature)\n",
        "\n",
        "    # Feature 9: Count 'https'\n",
        "    count_https_feature = count_https(url)\n",
        "    features.append(count_https_feature)\n",
        "\n",
        "    # Feature 10: Count 'http'\n",
        "    count_http_feature = count_http(url)\n",
        "    features.append(count_http_feature)\n",
        "\n",
        "    # Feature 11: Count '%'\n",
        "    count_per_feature = count_per(url)\n",
        "    features.append(count_per_feature)\n",
        "\n",
        "    # Feature 12: Count '?'\n",
        "    count_ques_feature = count_ques(url)\n",
        "    features.append(count_ques_feature)\n",
        "\n",
        "    # Feature 13: Count '-'\n",
        "    count_hyphen_feature = count_hyphen(url)\n",
        "    features.append(count_hyphen_feature)\n",
        "\n",
        "    # Feature 14: Count '='\n",
        "    count_equal_feature = count_equal(url)\n",
        "    features.append(count_equal_feature)\n",
        "\n",
        "    # Feature 15: URL Length\n",
        "    url_length_feature = url_length(url)\n",
        "    features.append(url_length_feature)\n",
        "\n",
        "    # Feature 16: Hostname Length\n",
        "    hostname_length_feature = hostname_length(url)\n",
        "    features.append(hostname_length_feature)\n",
        "\n",
        "    # Feature 17: Suspicious Words\n",
        "    suspicious_words_feature = suspicious_words(url)\n",
        "    features.append(suspicious_words_feature)\n",
        "\n",
        "    # Feature 18: Digit Count\n",
        "    digit_count_feature = digit_count(url)\n",
        "    features.append(digit_count_feature)\n",
        "\n",
        "    # Feature 19: Letter Count\n",
        "    letter_count_feature = letter_count(url)\n",
        "    features.append(letter_count_feature)\n",
        "\n",
        "    # Feature 20: FD Length\n",
        "    fd_length_feature = fd_length(url)\n",
        "    features.append(fd_length_feature)\n",
        "\n",
        "    # Feature 21: TLD Length\n",
        "    tld_length_feature = get_tld_length(url)\n",
        "    features.append(tld_length_feature)\n",
        "\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "iz1Y04OTRKjt"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "\n",
        "def get_prediction(url, model_path):\n",
        "\n",
        "    model = joblib.load(model_path)\n",
        "    url_features = feature_extractor(url)\n",
        "\n",
        "    print(\"Making prediction...\")\n",
        "    prediction = model.predict([url_features])\n",
        "    if prediction[0]==1:\n",
        "      print(\"Phishing\")\n",
        "    else:\n",
        "      print(\"Benign\")\n",
        "    #print(\"There is\", i, \"% chance that this URL is malicious!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iCUTdalbjTA0"
      },
      "outputs": [],
      "source": [
        "model_path = r\"/content/best_pipeline_url.pkl\"\n",
        "\n",
        "def url_predict(url):\n",
        "  prediction = get_prediction(url,model_path)\n",
        "  print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "best_pipeline = joblib.load('best_pipeline_tfidf.pkl')\n"
      ],
      "metadata": {
        "id": "IrNpqt1p7jF3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "email_pipeline = joblib.load('best_pipeline_tfidf.pkl')\n",
        "def preprocess(text):\n",
        "      words = word_tokenize(text)\n",
        "      no_stop_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "      return ' '.join(no_stop_words)\n",
        "\n",
        "def preprocess_email(email_text):\n",
        "\n",
        "  # Preprocess the text\n",
        "\n",
        "\n",
        "  preprocessed_email = preprocess(email_text)\n",
        "  preprocessed_email=preprocessed_email.lower()\n",
        "  return preprocessed_email\n",
        "print(stop_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWxGHsOJB1iR",
        "outputId": "0a11c7e8-696a-460c-ea67-19ac783e86da"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'from', 'he', 'me', 'down', \"doesn't\", 'his', 'off', 'any', 's', 'them', 'theirs', 'over', 'she', 'these', 'here', 'doesn', \"hasn't\", 'they', 'am', 'been', 'itself', \"couldn't\", 'its', \"won't\", \"shan't\", \"shouldn't\", 'in', 't', \"you've\", 'him', 'isn', 'between', 'mustn', 'who', 'nor', 'you', 'being', 'themselves', 'once', \"should've\", \"aren't\", 'very', 'is', 'my', 'we', 'o', 'both', \"it's\", \"didn't\", \"that'll\", 'are', 'most', 'not', 'just', 're', 'll', 'haven', 'if', 'wouldn', 'whom', 'didn', 'there', 'have', 'before', 'too', 'into', 'will', \"mustn't\", 'ourselves', 'where', 'when', 'has', 'about', 'mightn', 'ma', \"wouldn't\", 'than', 've', \"you're\", 'below', 'couldn', \"weren't\", 'up', 'own', 'few', 'above', 'hers', 'same', 'which', 'after', \"you'd\", 'on', 'hasn', 'under', 'a', \"don't\", \"haven't\", 'as', 'for', 'more', 'himself', 'her', 'yourselves', 'shouldn', 'then', 'ours', 'and', 'or', 'had', 'myself', 'only', 'ain', 'should', 'did', 'do', 'against', 'yourself', 'this', 'hadn', 'can', 'their', 'does', 'during', 'until', \"mightn't\", 'yours', 'further', 'needn', 'again', 'each', \"hadn't\", 'aren', 'don', 'won', 'shan', 'that', \"she's\", 'doing', 'while', 'by', 'with', 'out', 'some', 'no', 'weren', 'but', 'herself', 'the', 'other', 'why', \"isn't\", 'it', 'i', 'now', \"needn't\", 'what', 'were', 'an', 'at', 'how', 'was', 'y', 'your', 'to', 'those', 'wasn', 'because', 'such', 'our', 'through', \"wasn't\", 'be', 'm', \"you'll\", 'of', 'having', 'd', 'so', 'all'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def email_predict(email_text):\n",
        "  predicted_label = email_pipeline.predict([preprocess_email(email_text)])\n",
        "\n",
        "  # Print the predicted label (1 for Safe Email, 0 for Phishing Email)\n",
        "  if predicted_label[0] == '1':\n",
        "    print(\"Predicted Label: Safe Email\")\n",
        "  else:\n",
        "    print(\"Predicted Label: Phishing Email\")"
      ],
      "metadata": {
        "id": "EkdDCbJtB3zR"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final(text):\n",
        "  urls=check_url(text)\n",
        "  if len(urls)==0:\n",
        "    email_predict(text)\n",
        "  else:\n",
        "    for i in range(len(urls)):\n",
        "      url_predict(i)"
      ],
      "metadata": {
        "id": "ZgVojBKkP_g_"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=input()\n",
        "print(final(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBRDrhxnfCHf",
        "outputId": "0bdab838-d5e4-4b0c-e4c5-676cae6978e7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Respected Sir,  I am writing to you on behalf of the IEEE Student Branch at the National Institute of Technology Karnataka (NITK). We are excited to announce our upcoming IEEE Week, scheduled to take place from October 1st to 5th, where we aim to foster knowledge sharing and networking among students and professionals in the field of technology.  We are particularly honored to invite you to be a distinguished speaker at our event. Given your extensive experience as a member of the IEEE Executive Committee, we believe you are uniquely positioned to address a topic of utmost importance to our audience: \"Leveraging Technology for a Better Tomorrow.\"  We kindly request you to consider delivering a session that focuses on the positive impact of technology on society and the world at large. Your insights into how technological advancements can contribute to a more sustainable, equitable, and prosperous future will undoubtedly resonate with our students.  Your session could encompass various aspects, such as the role of technology in addressing global challenges, sustainable development, digital innovation, or any related subtopics that align with the overarching theme. We envision your talk as an opportunity to inspire and guide the next generation of technologists toward using their skills for the betterment of society.  Please let us know if you would be available and willing to participate in IEEE Week at NITK from October 1st to 5th, delivering a talk centered around \"Leveraging Technology for a Better Tomorrow.\" We would be more than happy to accommodate your preferred date and time.\n",
            "Predicted Label: Safe Email\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f"
      ],
      "metadata": {
        "id": "TWoioqNvhKXX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1S20SVsATFFnt-Ln7dqTULEM59Ev9dMuD",
      "authorship_tag": "ABX9TyMiGEQjkpGhqKqyty0w4PK6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}